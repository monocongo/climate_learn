{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_learn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/monocongo/model_learn/blob/master/notebooks/model_learn.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "74l7lcFQk4kT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup\n"
      ]
    },
    {
      "metadata": {
        "id": "ixh2Tyl1FHaj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this first cell we''ll load the necessary libraries and setup some logging and display options."
      ]
    },
    {
      "metadata": {
        "id": "JaCENoitkiXK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "from IPython import display\n",
        "from matplotlib import cm\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.data import Dataset\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = '{:.2f}'.format"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6tSwOT2bsUNM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pull data files from Google Drive\n"
      ]
    },
    {
      "metadata": {
        "id": "6dljBwZdE_Es",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Install PyDrive which will be used to access Google Drive and kick off the process to authorize the notebook running in the Google Colaboratory environment to touch our Drive files. When this cell executes it'll provide a link to authenticate into a Google Drive account to instatiate a PyDrive client. The Drive account that is selected should be one which has access to our all variables dataset file that we'll use for training/testing our model."
      ]
    },
    {
      "metadata": {
        "id": "SqSqvIptsV-t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        " \n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        " \n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eyG5gCxjseUQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we'll create dataset files within the Google Colaboratory environment corresponding to the flow and time tendency dataset files located on our Google Drive.\n"
      ]
    },
    {
      "metadata": {
        "id": "1LdCggflswj7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filename_h0 = 'fv091x180L26_dry_HS.cam.h0.2000-12-27-00000.nc'\n",
        "filename_h1 = 'fv091x180L26_dry_HS.cam.h1.2000-12-27-00000.nc'\n",
        "id_h0 = '1vptBPguIMU4FrvkC91xAd7_wOxoylBW0'\n",
        "id_h1 = '1ru8gmDKv8qPZGnfaTP2Sqv8Fsps48vAX'\n",
        "file_h0 = drive.CreateFile({'id': id_h0})  # creates a file in the Colab env using the ID for file <filename_h0>\n",
        "file_h1 = drive.CreateFile({'id': id_h1})  # creates a file in the Colab env using the ID for file <filename_h1>\n",
        "file_h0.GetContentFile(filename_h0)  # gets the file's contents and saves it as a local file named <filename_h0>\n",
        "file_h1.GetContentFile(filename_h1)  # gets the file's contents and saves it as a local file named <filename_h1>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y0gBz25Glf-3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we'll load our flow variables and time tendency forcings datasets into Xarray Dataset objects."
      ]
    },
    {
      "metadata": {
        "id": "_cC_-nNSlWIO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q xarray\n",
        "!pip install -U -q netCDF4\n",
        "\n",
        "import xarray as xr\n",
        "\n",
        "data_h0 = xr.open_dataset(filename_h0)\n",
        "data_h1 = xr.open_dataset(filename_h1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fLH3YQ2azUce",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define the features and configure feature columns\n"
      ]
    },
    {
      "metadata": {
        "id": "sh36g2stEz-l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In TensorFlow, we indicate a feature's data type using a construct called a feature column. Feature columns store only a description of the feature data; they do not contain the feature data itself. As features we'll use the following flow variables:\n",
        "\n",
        "* U (west-east (zonal) wind, m/s)\n",
        "* V (south-north (meridional) wind, m/s)\n",
        "* T (temperature, K)\n",
        "* PS (surface pressure, Pa)\n",
        "\n",
        "We'll take the flow variables dataset and trim out all but the above variables, and use this as the data source for features.\n",
        "\n",
        "The variables correspond to Numpy arrays, and we'll use the shapes of the variable arrays as the shapes of the corresponding feature columns.\n"
      ]
    },
    {
      "metadata": {
        "id": "Pv3DemYvmTh1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the input features as PS, T, U, and V.\n",
        "\n",
        "# remove all non-feature variables and unrelated coordinate variables from the DataSet, in order to trim the memory footprint.\n",
        "feature_vars = ['PS', 'T', 'U', 'V']\n",
        "feature_coord_vars = ['time', 'lev', 'lat', 'lon']\n",
        "for var in data_h0.variables:\n",
        "  if (var not in feature_vars) and (var not in feature_coord_vars):\n",
        "    data_h0 = data_h0.drop(var)\n",
        "  \n",
        "features = data_h0\n",
        "\n",
        "# Configure numeric feature columns for the input features.\n",
        "feature_columns = []\n",
        "for var in feature_vars:\n",
        "  feature_columns.append(tf.feature_column.numeric_column(var, shape=features[var].shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zoAKA7O6se2m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Display the flow variables (features) DataSet."
      ]
    },
    {
      "metadata": {
        "id": "7pk5TNd4sjVL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "fe96129b-6df1-4e07-ddfc-754cae9812cd"
      },
      "cell_type": "code",
      "source": [
        "features"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<xarray.Dataset>\n",
              "Dimensions:  (lat: 91, lev: 26, lon: 180, time: 720)\n",
              "Coordinates:\n",
              "  * lev      (lev) float64 3.545 7.389 13.97 23.94 37.23 53.11 70.06 85.44 ...\n",
              "  * time     (time) datetime64[ns] 2000-12-27 2000-12-27T00:30:00 ...\n",
              "  * lat      (lat) float64 -90.0 -88.0 -86.0 -84.0 -82.0 -80.0 -78.0 -76.0 ...\n",
              "  * lon      (lon) float64 0.0 2.0 4.0 6.0 8.0 10.0 12.0 14.0 16.0 18.0 20.0 ...\n",
              "Data variables:\n",
              "    PS       (time, lat, lon) float32 ...\n",
              "    T        (time, lev, lat, lon) float32 ...\n",
              "    U        (time, lev, lat, lon) float32 ...\n",
              "    V        (time, lev, lat, lon) float32 ...\n",
              "Attributes:\n",
              "    Conventions:      CF-1.0\n",
              "    source:           CAM\n",
              "    case:             fv091x180L26_dry_HS\n",
              "    title:            CAM5-FV 2x2L26, dry HS\n",
              "    logname:          cjablono\n",
              "    host:             r1i3n29\n",
              "    Version:          $Name$\n",
              "    revision_Id:      $Id$\n",
              "    initial_file:     /glade2/scratch2/cjablono/fv091x180L26_dry_HS/fv091x180...\n",
              "    topography_file:  /glade/p/work/cjablono/dycore_initial_data/dcmip_james/..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "cIEYStXorUUp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a5c5e2a7-d4ca-4b93-897b-d3ecd2d6900b"
      },
      "cell_type": "code",
      "source": [
        "feature_columns"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[_NumericColumn(key='PS', shape=(720, 91, 180), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " _NumericColumn(key='T', shape=(720, 26, 91, 180), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " _NumericColumn(key='U', shape=(720, 26, 91, 180), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " _NumericColumn(key='V', shape=(720, 26, 91, 180), default_value=None, dtype=tf.float32, normalizer_fn=None)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "ADi7KQHtBKMf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define the targets (labels)\n"
      ]
    },
    {
      "metadata": {
        "id": "VjEZ2K5HEF6t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Time tendency forcings are the targets (labels) that our model should learn to predict.\n",
        "\n",
        "* PTTEND (time tendency of the temperature)\n",
        "* PUTEND (time tendency of the zonal wind)\n",
        "* PVTEND (time tendency of the meridional wind)\n",
        "\n",
        "We'll take the time tendency forcings dataset and trim out all other variables so we can use this as the data source for targets. "
      ]
    },
    {
      "metadata": {
        "id": "hN19tItj6hrN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the targets (labels) as PTTEND, PUTEND, and PVTEND.\n",
        "\n",
        "# Remove all non-target variables and unrelated coordinate variables from the DataSet, in order to trim the memory footprint.\n",
        "target_vars = ['PTTEND', 'PUTEND', 'PVTEND']\n",
        "target_coord_vars = ['time', 'lev', 'lat', 'lon']\n",
        "for var in data_h1.variables:\n",
        "  if (var not in target_vars) and (var not in target_coord_vars):\n",
        "    data_h1 = data_h1.drop(var)\n",
        "targets = data_h1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bUXvCOIgsqx1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Display the time tendency forcings (targets/labels) dataset."
      ]
    },
    {
      "metadata": {
        "id": "DM8R4w3fszNv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "fbf6b4d6-30ca-4d60-b8c4-070e5edcd4dd"
      },
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<xarray.Dataset>\n",
              "Dimensions:  (lat: 91, lev: 26, lon: 180, time: 720)\n",
              "Coordinates:\n",
              "  * lev      (lev) float64 3.545 7.389 13.97 23.94 37.23 53.11 70.06 85.44 ...\n",
              "  * time     (time) datetime64[ns] 2000-12-27 2000-12-27T00:30:00 ...\n",
              "  * lat      (lat) float64 -90.0 -88.0 -86.0 -84.0 -82.0 -80.0 -78.0 -76.0 ...\n",
              "  * lon      (lon) float64 0.0 2.0 4.0 6.0 8.0 10.0 12.0 14.0 16.0 18.0 20.0 ...\n",
              "Data variables:\n",
              "    PTTEND   (time, lev, lat, lon) float32 ...\n",
              "    PUTEND   (time, lev, lat, lon) float32 ...\n",
              "    PVTEND   (time, lev, lat, lon) float32 ...\n",
              "Attributes:\n",
              "    Conventions:      CF-1.0\n",
              "    source:           CAM\n",
              "    case:             fv091x180L26_dry_HS\n",
              "    title:            CAM5-FV 2x2L26, dry HS\n",
              "    logname:          cjablono\n",
              "    host:             r1i3n29\n",
              "    Version:          $Name$\n",
              "    revision_Id:      $Id$\n",
              "    initial_file:     /glade2/scratch2/cjablono/fv091x180L26_dry_HS/fv091x180...\n",
              "    topography_file:  /glade/p/work/cjablono/dycore_initial_data/dcmip_james/..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "7YNrGbEeNN2c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Confirm the compatability of our features and targets datasets, in terms of dimensions and coordinates, to provide an initial sanity check. "
      ]
    },
    {
      "metadata": {
        "id": "P5RsmkV9IvZQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if features.dims != targets.dims:\n",
        "    print(\"WARNING: Unequal dimensions\")\n",
        "else:\n",
        "    for coord in features.coords:\n",
        "        if not (features.coords[coord] == targets.coords[coord]).all():\n",
        "            print(\"WARNING: Unequal {} coordinates\".format(coord))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J-yXL6dZMWjP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Split the data into training, validation, and testing datasets"
      ]
    },
    {
      "metadata": {
        "id": "ApCinhR8__SL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll initially split the dataset into training, validation, and testing datasets with 50% for training and 25% each for validation and testing. We'll use the longitude dimension to split since it has 180 points and divides evenly by four. We get every other longitude starting at the first longitude to get 50% of the dataset for training, then every fourth longitude starting at the second longitude to get 25% of the dataset for validation, and every fourth longitude starting at the fourth longitude to get 25% of the dataset for testing."
      ]
    },
    {
      "metadata": {
        "id": "Q6waMx-cMg71",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lon_range_training = list(range(0, features.dims['lon'], 2))\n",
        "lon_range_validation = list(range(1, features.dims['lon'], 4))\n",
        "lon_range_testing = list(range(3, features.dims['lon'], 4))\n",
        "\n",
        "features_training = features.isel(lon=lon_range_training)\n",
        "features_validation = features.isel(lon=lon_range_validation)\n",
        "features_testing = features.isel(lon=lon_range_testing)\n",
        "\n",
        "targets_training = targets.isel(lon=lon_range_training)\n",
        "targets_validation = targets.isel(lon=lon_range_validation)\n",
        "targets_testing = targets.isel(lon=lon_range_testing)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cHEqkPkjCPDm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create the neural network\n"
      ]
    },
    {
      "metadata": {
        "id": "nUHTM6yaDzZV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we'll instantiate and configure a neural network using TensorFlow's [DNNRegressor](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNRegressor) class. We'll train this model using the GradientDescentOptimizer, which implements Mini-Batch Stochastic Gradient Descent (SGD). The learning_rate argument controls the size of the gradient step.\n",
        "\n",
        "NOTE: To be safe, we also apply gradient clipping to our optimizer via `clip_gradients_by_norm`. Gradient clipping ensures the magnitude of the gradients do not become too large during training, which can cause gradient descent to fail.\n",
        "\n",
        "We use `hidden_units`to define the structure of the NN. The `hidden_units` argument provides a list of ints, where each int corresponds to a hidden layer and indicates the number of nodes in it. For example, consider the following assignment:\n",
        "\n",
        "`hidden_units=[3, 10]`\n",
        "\n",
        "The preceding assignment specifies a neural net with two hidden layers:\n",
        "\n",
        "The first hidden layer contains 3 nodes.\n",
        "The second hidden layer contains 10 nodes.\n",
        "If we wanted to add more layers, we'd add more ints to the list. For example, `hidden_units=[10, 20, 30, 40]` would create four layers with ten, twenty, thirty, and forty units, respectively.\n",
        "\n",
        "By default, all hidden layers will use ReLu activation and will be fully connected."
      ]
    },
    {
      "metadata": {
        "id": "Cmvlnh4uC9SS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use gradient descent as the optimizer for training the model.\n",
        "gd_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0000001)\n",
        "gd_optimizer = tf.contrib.estimator.clip_gradients_by_norm(gd_optimizer, 5.0)\n",
        "\n",
        "# Use two hidden layers with 3 and 10 nodes each.\n",
        "hidden_units=[3, 4]\n",
        "\n",
        "# Instantiate the neural network.\n",
        "dnn_regressor = tf.estimator.DNNRegressor(feature_columns=feature_columns,\n",
        "                                          hidden_units=hidden_units,\n",
        "                                          optimizer=gd_optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L1CRW1a0Ds1C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define the input function"
      ]
    },
    {
      "metadata": {
        "id": "rhyUxyMoF0wQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To import our weather data into our DNNRegressor, we need to define an input function, which instructs TensorFlow how to preprocess the data, as well as how to batch, shuffle, and repeat it during model training.\n",
        "\n",
        "First, we'll convert our xarray feature data into a dict of NumPy arrays. We can then use the TensorFlow Dataset API to construct a dataset object from our data, and then break our data into batches of `batch_size`, to be repeated for the specified number of epochs (`num_epochs`).\n",
        "\n",
        "NOTE: When the default value of `num_epochs=None` is passed to `repeat()`, the input data will be repeated indefinitely.\n",
        "\n",
        "Next, if `shuffle` is set to True, we'll shuffle the data so that it's passed to the model randomly during training. The `buffer_size` argument specifies the size of the dataset from which shuffle will randomly sample.\n",
        "\n",
        "Finally, our input function constructs an iterator for the dataset and returns the next batch of data."
      ]
    },
    {
      "metadata": {
        "id": "-ZiWqTxvGNJO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.data import Dataset\n",
        "\n",
        "def get_input(features, \n",
        "              targets, \n",
        "              batch_size=1, \n",
        "              shuffle=True, \n",
        "              num_epochs=None):\n",
        "    \"\"\"\n",
        "    Extracts a batch of elements from a dataset.\n",
        "  \n",
        "    Args:\n",
        "      features: xarray Dataset of features\n",
        "      targets: xarray Dataset of targets\n",
        "      batch_size: Size of batches to be passed to the model\n",
        "      shuffle: True or False. Whether to shuffle the data.\n",
        "      num_epochs: Number of epochs for which data should be repeated. \n",
        "                  None == repeat indefinitely\n",
        "    Returns:\n",
        "      Tuple of (features, labels) for next data batch\n",
        "    \"\"\"\n",
        "  \n",
        "    # Convert xarray data into a dict of numpy arrays.\n",
        "    features = {key:np.array(value) for key,value in dict(features).items()}                                           \n",
        " \n",
        "    # Construct a dataset, and configure batching/repeating.\n",
        "    ds = Dataset.from_tensor_slices((features, targets)) # warning: 2GB limit\n",
        "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
        "    \n",
        "    # Shuffle the data, if specified.\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=10000)\n",
        "    \n",
        "    # Return the next batch of data.\n",
        "    features, labels = ds.make_one_shot_iterator().get_next()\n",
        "    return features, labels\n",
        "\n",
        "# Create input functions. Wrap get_input() in a lambda so we \n",
        "# can pass in features and targets as arguments.\n",
        "input_training = lambda: get_input(features_training, \n",
        "                                   targets_training, \n",
        "                                   batch_size=10)\n",
        "predict_input_training = lambda: get_input(features_training, \n",
        "                                           targets_training, \n",
        "                                           num_epochs=1, \n",
        "                                           shuffle=False)\n",
        "predict_input_validation = lambda: get_input(features_validation, \n",
        "                                             targets_validation, \n",
        "                                             num_epochs=1, \n",
        "                                             shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oqJj8vtMIbt5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train and evaluate the model"
      ]
    },
    {
      "metadata": {
        "id": "jQTKlqSHIgVh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can now call `train()` on our `dnn_regressor` to train the model. We'll loop over a number of periods and on each loop we'll train the model, use it to make predictions, and compute the RMSE of the loss for both training and validation datasets."
      ]
    },
    {
      "metadata": {
        "id": "iwfgGEY-I1pR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "0471a7f1-211b-4412-dd69-61bb8ca2386c"
      },
      "cell_type": "code",
      "source": [
        "print(\"Training model...\")\n",
        "print(\"RMSE (on training data):\")\n",
        "training_rmse = []\n",
        "validation_rmse = []\n",
        "\n",
        "steps = 500\n",
        "periods = 20\n",
        "steps_per_period = steps / periods\n",
        "\n",
        "# Train the model inside a loop so that we can periodically assess loss metrics.\n",
        "for period in range (0, periods):\n",
        "\n",
        "    # Train the model, starting from the prior state.\n",
        "    dnn_regressor.train(input_fn=input_training,\n",
        "                        steps=steps_per_period)\n",
        "\n",
        "    # Take a break and compute predictions, converting to numpy arrays.\n",
        "    training_predictions = dnn_regressor.predict(input_fn=predict_input_training)\n",
        "    training_predictions = np.array([item['predictions'][0] for item in training_predictions])\n",
        "    \n",
        "    validation_predictions = dnn_regressor.predict(input_fn=predict_input_validation)\n",
        "    validation_predictions = np.array([item['predictions'][0] for item in validation_predictions])\n",
        "    \n",
        "    # Compute training and validation loss.\n",
        "    training_root_mean_squared_error = math.sqrt(\n",
        "        metrics.mean_squared_error(training_predictions, targets_training))\n",
        "    validation_root_mean_squared_error = math.sqrt(\n",
        "        metrics.mean_squared_error(validation_predictions, targets_validation))\n",
        "    \n",
        "    # Print the current loss.\n",
        "    print(\"  period %02d : %0.2f\" % (period, training_root_mean_squared_error))\n",
        "    \n",
        "    # Add the loss metrics from this period to our list.\n",
        "    training_rmse.append(training_root_mean_squared_error)\n",
        "    validation_rmse.append(validation_root_mean_squared_error)\n",
        "\n",
        "print(\"Model training finished.\")\n",
        "\n",
        "# Output a graph of loss metrics over periods.\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.xlabel(\"Periods\")\n",
        "plt.title(\"Root Mean Squared Error vs. Periods\")\n",
        "plt.tight_layout()\n",
        "plt.plot(training_rmse, label=\"training\")\n",
        "plt.plot(validation_rmse, label=\"validation\")\n",
        "plt.legend()\n",
        "\n",
        "print(\"Final RMSE (on training data):   %0.2f\" % training_root_mean_squared_error)\n",
        "print(\"Final RMSE (on validation data): %0.2f\" % validation_root_mean_squared_error)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model...\n",
            "RMSE (on training data):\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/_collections_abc.py:720: FutureWarning: iteration over an xarray.Dataset will change in xarray v0.11 to only include data variables, not coordinates. Iterate over the Dataset.variables property instead to preserve existing behavior in a forwards compatible manner.\n",
            "  yield from self._mapping\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "wVzN6_fWZDJn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}